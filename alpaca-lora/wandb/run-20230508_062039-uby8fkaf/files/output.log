  0%|          | 0/72 [00:00<?, ?it/s]/home/tricorder/yang/anaconda3/envs/llama_alpaca_lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 14%|█▍        | 10/72 [2:38:38<16:10:37, 939.31s/it]










 28%|██▊       | 20/72 [6:31:18<18:58:27, 1313.60s/it]










 42%|████▏     | 30/72 [12:13:32<23:12:26, 1989.20s/it]










 56%|█████▌    | 40/72 [18:07:09<18:50:01, 2118.79s/it]










 69%|██████▉   | 50/72 [22:09:35<7:39:42, 1253.77s/it]









 82%|████████▏ | 59/72 [25:02:36<4:21:09, 1205.34s/it]










 96%|█████████▌| 69/72 [28:35:24<1:03:03, 1261.01s/it]


 99%|█████████▊| 71/72 [29:18:52<21:23, 1283.31s/it]
{'train_runtime': 106727.3492, 'train_samples_per_second': 1.406, 'train_steps_per_second': 0.001, 'train_loss': 2.3480590184529624, 'epoch': 2.95}
100%|██████████| 72/72 [29:38:42<00:00, 1482.25s/it]
ERROR: Could not consume arg:
Usage: finetune.py '' '' --base_model /home/tricorder/test/gxc/decapoda-research/llama-7b-hf '' '' --data_path ../data/alpaca_data.json '' '' --output_dir ../lora_alpaca_output_free/source_2048 '' '' --batch_size 2048 '' '' --micro_batch_size 4 '' '' --num_epochs 3 '' '' --learning_rate 2e-5 '' '' --cutoff_len 512 '' '' --val_set_size 2000 '' '' --lora_r
For detailed information on this command, run:
  finetune.py '' '' --base_model /home/tricorder/test/gxc/decapoda-research/llama-7b-hf '' '' --data_path ../data/alpaca_data.json '' '' --output_dir ../lora_alpaca_output_free/source_2048 '' '' --batch_size 2048 '' '' --micro_batch_size 4 '' '' --num_epochs 3 '' '' --learning_rate 2e-5 '' '' --cutoff_len 512 '' '' --val_set_size 2000 '' '' --lora_r --help