  0%|          | 0/240 [00:00<?, ?it/s]/home/tricorder/yang/anaconda3/envs/llama_alpaca_lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









  4%|▍         | 10/240 [1:19:30<28:24:09, 444.56s/it]










  8%|▊         | 20/240 [2:37:32<27:18:59, 447.00s/it]









 12%|█▏        | 29/240 [3:49:06<28:04:59, 479.15s/it]










 16%|█▋        | 39/240 [5:03:39<27:39:54, 495.49s/it]











 21%|██        | 50/240 [6:27:25<26:04:10, 493.95s/it]









 25%|██▍       | 59/240 [8:09:18<40:37:32, 808.02s/it]










 29%|██▉       | 69/240 [10:05:10<25:09:26, 529.63s/it]










 33%|███▎      | 79/240 [12:51:21<44:42:25, 999.66s/it]











 38%|███▊      | 90/240 [16:03:50<43:01:50, 1032.74s/it]









 41%|████▏     | 99/240 [18:41:36<43:50:40, 1119.44s/it]










 45%|████▌     | 109/240 [21:29:55<35:36:07, 978.38s/it]











 50%|█████     | 120/240 [24:26:38<24:04:09, 722.08s/it]










 54%|█████▍    | 130/240 [26:01:05<17:30:38, 573.08s/it]









