  0%|          | 0/240 [00:00<?, ?it/s]/home/tricorder/yang/anaconda3/envs/llama_alpaca_lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









  4%|▍         | 10/240 [1:36:23<34:05:33, 533.62s/it]










  8%|▊         | 20/240 [3:12:55<32:59:15, 539.80s/it]










 12%|█▎        | 30/240 [5:01:48<38:54:51, 667.10s/it]










 17%|█▋        | 40/240 [6:45:02<34:45:41, 625.71s/it]










 21%|██        | 50/240 [8:29:46<34:51:29, 660.47s/it]










 25%|██▌       | 60/240 [9:50:41<23:32:39, 470.89s/it]










 29%|██▉       | 70/240 [11:13:35<20:26:22, 432.84s/it]










 33%|███▎      | 80/240 [12:34:02<18:28:13, 415.58s/it]










 38%|███▊      | 90/240 [13:35:21<14:51:45, 356.70s/it]










 42%|████▏     | 100/240 [14:40:31<18:03:10, 464.22s/it]










 46%|████▌     | 110/240 [15:40:24<14:26:10, 399.77s/it]










 50%|█████     | 120/240 [16:39:37<9:45:10, 292.59s/it]









 54%|█████▍    | 129/240 [17:22:19<8:21:26, 271.05s/it]










 58%|█████▊    | 139/240 [18:08:10<8:00:03, 285.18s/it]










 62%|██████▏   | 149/240 [18:53:40<7:40:20, 303.52s/it]











 67%|██████▋   | 160/240 [19:43:01<6:28:36, 291.45s/it]









 70%|███████   | 169/240 [20:22:01<4:48:26, 243.75s/it]










 75%|███████▍  | 179/240 [21:08:43<4:24:53, 260.55s/it]










 79%|███████▉  | 189/240 [21:54:34<3:54:49, 276.26s/it]










 83%|████████▎ | 199/240 [22:40:16<3:23:23, 297.64s/it]
 83%|████████▎ | 200/240 [22:44:54<3:14:25, 291.65s/it]









































 98%|█████████▊| 246/250 [01:23<00:01,  3.02it/s]

  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 88%|████████▊ | 210/240 [23:31:07<2:29:06, 298.22s/it]






